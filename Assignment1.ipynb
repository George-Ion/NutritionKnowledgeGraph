{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f62633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install rdflib pandas spacy vaderSentiment requests\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e3adf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "import requests\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from rdflib import Graph, Namespace, URIRef, Literal\n",
    "from rdflib.namespace import RDF, RDFS, OWL, XSD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0b08c7",
   "metadata": {},
   "source": [
    "### Please define the path files here ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3037536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to the data files and outpout files\n",
    "TEAM_ID = \"12\"\n",
    "BASE_PATH = r\"Put yout path here\"  # <-- Update this to your actual base path\n",
    "\n",
    "# Data files\n",
    "NUTRITION_CSV = BASE_PATH + r\"\\data\\Nutrition.csv\"\n",
    "RECIPES_CSV = BASE_PATH + r\"\\data\\Recipes.csv\"\n",
    "RESTAURANTS_CSV = BASE_PATH + r\"\\data\\Restaurants.csv\"\n",
    "REVIEWS_TXT = BASE_PATH + r\"\\data\\Reviews.txt\"\n",
    "\n",
    "# Output files (as per assignment requirements)\n",
    "STRUCTURED_KG = BASE_PATH + f\"\\\\KEN4256-structured-KG-{TEAM_ID}.ttl\"\n",
    "UNSTRUCTURED_KG = BASE_PATH + f\"\\\\KEN4256-unstructured-KG-{TEAM_ID}.ttl\"\n",
    "INTEGRATED_KG = BASE_PATH + f\"\\\\KEN4256-integrated-KG-{TEAM_ID}.ttl\"\n",
    "SPARQL_RESULTS = BASE_PATH + \"\\\\SPARQL_Results.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4605b8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph()\n",
    "\n",
    "# Using schema.org as the namespace for our knowledge graph\n",
    "SDO = Namespace(\"https://schema.org/\")\n",
    "g.bind(\"schema\", SDO)\n",
    "\n",
    "# Classes\n",
    "g.add((SDO.Recipe, RDF.type, OWL.Class))\n",
    "g.add((SDO.Review, RDF.type, OWL.Class))\n",
    "g.add((SDO.Nutrition, RDF.type, OWL.Class))\n",
    "g.add((SDO.Restaurant, RDF.type, OWL.Class))\n",
    "g.add((SDO.Ingredient, RDF.type, OWL.Class))  \n",
    "g.add((SDO.Cuisine, RDF.type, OWL.Class))\n",
    "\n",
    "# Recipe properties\n",
    "g.add((SDO.image, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((SDO.image, RDFS.domain, SDO.Recipe))\n",
    "g.add((SDO.image, RDFS.range, XSD.anyURI))\n",
    "\n",
    "g.add((SDO.keywords, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((SDO.keywords, RDFS.domain, SDO.Recipe))\n",
    "g.add((SDO.keywords, RDFS.range, XSD.string))\n",
    "\n",
    "g.add((SDO.datePublished, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((SDO.datePublished, RDFS.domain, SDO.Recipe))\n",
    "g.add((SDO.datePublished, RDFS.range, XSD.dateTime))\n",
    "\n",
    "g.add((SDO.cookTime, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((SDO.cookTime, RDFS.domain, SDO.Recipe))\n",
    "g.add((SDO.cookTime, RDFS.range, XSD.string))\n",
    "\n",
    "g.add((SDO.ingredients, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((SDO.ingredients, RDFS.domain, SDO.Recipe))\n",
    "g.add((SDO.ingredients, RDFS.range, XSD.string))\n",
    "\n",
    "g.add((SDO.givenName, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((SDO.givenName, RDFS.domain, SDO.Recipe))\n",
    "g.add((SDO.givenName, RDFS.range, XSD.string))\n",
    "\n",
    "g.add((SDO.recipeCategory, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((SDO.recipeCategory, RDFS.domain, SDO.Recipe))\n",
    "g.add((SDO.recipeCategory, RDFS.range, XSD.string))\n",
    "\n",
    "# Recipe object properties\n",
    "g.add((SDO.hasNutrition, RDF.type, OWL.ObjectProperty))\n",
    "g.add((SDO.hasNutrition, RDFS.domain, SDO.Recipe))\n",
    "g.add((SDO.hasNutrition, RDFS.range, SDO.Nutrition))\n",
    "\n",
    "g.add((SDO.servesCuisine, RDF.type, OWL.ObjectProperty))\n",
    "g.add((SDO.servesCuisine, RDFS.domain, SDO.Restaurant))\n",
    "g.add((SDO.servesCuisine, RDFS.domain, SDO.Recipe))  \n",
    "g.add((SDO.servesCuisine, RDFS.range, SDO.Cuisine))\n",
    "\n",
    "# Recipe to Ingredient relationship (Task 3.3)\n",
    "g.add((SDO.recipeIngredient, RDF.type, OWL.ObjectProperty))\n",
    "g.add((SDO.recipeIngredient, RDFS.domain, SDO.Recipe))\n",
    "g.add((SDO.recipeIngredient, RDFS.range, SDO.Ingredient))\n",
    "\n",
    "g.add((SDO.UserReview, RDF.type, OWL.ObjectProperty))\n",
    "g.add((SDO.UserReview, RDFS.domain, SDO.Review))\n",
    "g.add((SDO.UserReview, RDFS.range, SDO.Recipe))\n",
    "\n",
    "# Nutrition properties\n",
    "g.add((SDO.calories, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((SDO.calories, RDFS.domain, SDO.Nutrition))\n",
    "g.add((SDO.calories, RDFS.range, XSD.decimal))\n",
    "\n",
    "g.add((SDO.sugarContent, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((SDO.sugarContent, RDFS.domain, SDO.Nutrition))\n",
    "g.add((SDO.sugarContent, RDFS.range, XSD.decimal))\n",
    "\n",
    "g.add((SDO.proteinContent, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((SDO.proteinContent, RDFS.domain, SDO.Nutrition))\n",
    "g.add((SDO.proteinContent, RDFS.range, XSD.decimal))\n",
    "\n",
    "g.add((SDO.fiberContent, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((SDO.fiberContent, RDFS.domain, SDO.Nutrition))\n",
    "g.add((SDO.fiberContent, RDFS.range, XSD.decimal))\n",
    "\n",
    "g.add((SDO.name, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((SDO.name, RDFS.domain, SDO.Nutrition))\n",
    "g.add((SDO.name, RDFS.range, XSD.string))\n",
    "\n",
    "# Restaurant properties\n",
    "g.add((SDO.AggregateRating, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((SDO.AggregateRating, RDFS.domain, SDO.Restaurant))\n",
    "g.add((SDO.AggregateRating, RDFS.range, XSD.decimal))\n",
    "\n",
    "g.add((SDO.priceRange, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((SDO.priceRange, RDFS.domain, SDO.Restaurant))\n",
    "g.add((SDO.priceRange, RDFS.range, XSD.decimal))\n",
    "\n",
    "g.add((SDO.city, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((SDO.city, RDFS.domain, SDO.Restaurant))\n",
    "g.add((SDO.city, RDFS.range, XSD.string))\n",
    "\n",
    "g.add((SDO.country, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((SDO.country, RDFS.domain, SDO.Restaurant))\n",
    "g.add((SDO.country, RDFS.range, XSD.string))\n",
    "\n",
    "g.add((SDO.orderDelivery, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((SDO.orderDelivery, RDFS.domain, SDO.Restaurant))\n",
    "g.add((SDO.orderDelivery, RDFS.range, XSD.integer))\n",
    "\n",
    "g.add((SDO.legalName, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((SDO.legalName, RDFS.domain, SDO.Restaurant))\n",
    "g.add((SDO.legalName, RDFS.range, XSD.string))\n",
    "\n",
    "# Review properties\n",
    "g.add((SDO.reviewRating, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((SDO.reviewRating, RDFS.domain, SDO.Review))\n",
    "g.add((SDO.reviewRating, RDFS.range, XSD.string))\n",
    "\n",
    "\n",
    "g.add((SDO.reviewCount, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((SDO.reviewCount, RDFS.domain, SDO.Review))\n",
    "g.add((SDO.reviewCount, RDFS.range, XSD.integer))\n",
    "\n",
    "\n",
    "\n",
    "# Using givenName , name and legalName as properties for the name of the recipe, nutrition and restaurant respectively\n",
    "# so we unbind the default name property from schema.org to avoid confusion and ensure that we use the correct properties for each class\n",
    "\n",
    "# Serializing the knowledge graph to a turtle file so we can visualize it with https://service.tib.eu/webvowl/ \n",
    "g.serialize(\"food_nutrition_kg.ttl\", format=\"turtle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b462b0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using custom namespaces for our knowledge graph, because if we just use the fod namespace for everything  the recipe and nutrition \n",
    "# URIs colide in the namespace and the instances of recipes and nutrition become one. By using separate namespaces \n",
    "# we can better structure our knowledge graph and make it easier to understand and query\n",
    "FOD = Namespace(\"http://kg-course.io/food-nutrition/\")\n",
    "g.bind(\"fod\", FOD)  \n",
    "NUTR = Namespace(\"http://kg-course.io/food-nutrition/nutrition/\")\n",
    "g.bind(\"nutr\", NUTR)\n",
    "REC = Namespace(\"http://kg-course.io/food-nutrition/recipe/\")\n",
    "g.bind(\"rec\", REC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6000ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create safe URIs by removing/encoding problematic characters, because we hade issues with special characters \n",
    "# in the data causing problems when creating URIs for the instances\n",
    "def clean_for_uri(name):\n",
    "    \"\"\"Clean a string to make it safe for use in a URI.\"\"\"\n",
    "    if pd.isna(name):\n",
    "        return \"Unknown\"\n",
    "    name = str(name).strip()\n",
    "    # Remove quotes and parentheses\n",
    "    name = name.replace('\"', '').replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    # Replace spaces and commas with underscores\n",
    "    name = name.replace(\" \", \"_\").replace(\",\", \"_\")\n",
    "    # Remove or replace other problematic characters\n",
    "    name = re.sub(r'[{}%#?&/\\\\<>|:*]', '', name)\n",
    "    # Remove any non-ASCII characters that could cause issues\n",
    "    name = name.encode('ascii', 'ignore').decode('ascii')\n",
    "    # Remove multiple underscores\n",
    "    name = re.sub(r'_+', '_', name)\n",
    "    # Remove leading/trailing underscores\n",
    "    name = name.strip('_')\n",
    "    return name if name else \"Unknown\"\n",
    "\n",
    "#Creating all data frames with pandas so we can easily go through them and add data to the knowledge graph\n",
    "nutrion_df = pd.read_csv(\n",
    "    NUTRITION_CSV,\n",
    "    sep=\";\",\n",
    "    encoding=\"latin1\"\n",
    ")\n",
    "\n",
    "print(nutrion_df.head())\n",
    "\n",
    "# Had trouble with data from recipes file, because of multi-line cells. So we use the Python engine and specify the quote character\n",
    "recipes_df = pd.read_csv(\n",
    "    RECIPES_CSV,\n",
    "    sep=\";\",\n",
    "    encoding=\"latin1\",\n",
    "    #Trouble with multi-line cells, so we use the Python engine and specify the quote character to handle the c(\"\") blocks properly\n",
    "    engine='python',        \n",
    "    quotechar='\"',          \n",
    "    # default is doublequote=True, but we specify it explicitly to ensure that the \"\" inside the c(\"\") blocks are handled correctly\n",
    "    doublequote=True,       \n",
    "    # In case of a bad line we skip it\n",
    "    on_bad_lines='skip'     \n",
    ")\n",
    "\n",
    "print(recipes_df.head())\n",
    "\n",
    "\n",
    "restaurants_df = pd.read_csv(\n",
    "    RESTAURANTS_CSV,\n",
    "    sep=\";\",\n",
    "    encoding=\"latin1\", \n",
    ")\n",
    "\n",
    "#restaurants_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbef2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Itterate over the nutrition data to populate the knowledge graph with them\n",
    "for _,row in nutrion_df.iterrows():\n",
    "\n",
    "    # Transform the names into URIs\n",
    "    clean_name = clean_for_uri(row[\"Name\"])\n",
    "    # Using \"http://kg-course.io/food-nutrition/nutrition\" as the namespace for the food items \n",
    "    # We create a URI for each food item and add it to the graph as an instance of the Nutrition class\n",
    "    nutrition_uri = NUTR[clean_name]\n",
    "    # Add the nutrition information to the graph as RDF triples, name, calories, sugar content, protein content, and fiber content are added\n",
    "    # as properties from schema.org\n",
    "    g.add((nutrition_uri, RDF.type, SDO.Nutrition))\n",
    "    g.add((nutrition_uri, SDO.name, Literal(row[\"Name\"], datatype=XSD.string)))\n",
    "    g.add((nutrition_uri, SDO.calories, Literal(row[\"Calories\"], datatype=XSD.decimal)))\n",
    "    g.add((nutrition_uri, SDO.sugarContent, Literal(row[\"SugarContent\"], datatype=XSD.decimal)))\n",
    "    g.add((nutrition_uri, SDO.proteinContent, Literal(row[\"ProteinContent\"], datatype=XSD.decimal)))\n",
    "    g.add((nutrition_uri, SDO.fiberContent, Literal(row[\"FiberContent\"], datatype=XSD.decimal)))\n",
    "\n",
    "    # We only take the first 10000 rows to manage computational complexity and avoid scalability issuses\n",
    "    if row.name > 10000:\n",
    "        break\n",
    "\n",
    "print(g.serialize(format='ttl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4bed1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the first image from the urls so we can output it in the sparql querry at the end\n",
    "def extract_first_image_url(images_str):\n",
    "    if pd.isna(images_str) or str(images_str).strip() == \"\" or \"character(0)\" in str(images_str):\n",
    "        return \"\"\n",
    "    images_str = str(images_str)\n",
    "    # Find URLs inside double quotes to preserve commas in URL \n",
    "    urls = re.findall(r'\"(https?://[^\"]+)\"', images_str)\n",
    "    if urls:\n",
    "        return urls[0]\n",
    "    return \"\"\n",
    "\n",
    "print(\"This part is computationally intensive and may take a few minutes\")\n",
    "for _,row in recipes_df.iterrows():\n",
    "\n",
    "    # Transform the names into URIs as above\n",
    "    clean_name = clean_for_uri(row[\"Name\"])\n",
    "    recipe_uri = REC[clean_name]\n",
    "    \n",
    "    # Bind the recipe data to the knowledge graph by adding RDF triples for the recipe properties, such as recipe name(givenName), image, keywords, date published, cook time, and ingredients\n",
    "    g.add((recipe_uri, RDF.type, SDO.Recipe))\n",
    "    g.add((recipe_uri, SDO.givenName, Literal(row[\"Name\"], datatype=XSD.string)))\n",
    "    g.add((recipe_uri, SDO.image, Literal(extract_first_image_url(row[\"Images\"]), datatype=XSD.anyURI)))\n",
    "    g.add((recipe_uri, SDO.keywords, Literal(row[\"Keywords\"], datatype=XSD.string)))\n",
    "    g.add((recipe_uri, SDO.datePublished, Literal(row[\"DatePublished\"], datatype=XSD.dateTime)))\n",
    "    g.add((recipe_uri, SDO.cookTime, Literal(row[\"CookTime\"], datatype=XSD.string)))\n",
    "    g.add((recipe_uri, SDO.ingredients, Literal(row[\"RecipeIngredientParts\"], datatype=XSD.string)))\n",
    "    g.add((recipe_uri, SDO.recipeCategory, Literal(row[\"RecipeCategory\"], datatype=XSD.string)))\n",
    "\n",
    "\n",
    "    # Link the recipe to nutrition information based on the name of the recipe and the name of the nutrition \n",
    "    # If the name of the recipe contains the name of a nutrition item, we link them with the hasNutrition property. \n",
    "    if row[\"Name\"] in nutrion_df[\"Name\"].values:\n",
    "        clean_nutr_name = clean_for_uri(row[\"Name\"])\n",
    "        nutrition_uri = NUTR[clean_nutr_name]\n",
    "        g.add((recipe_uri, SDO.hasNutrition, nutrition_uri))\n",
    "\n",
    "    # Link the recipe to cuisines based on the keywords and the cuisines in the restaurants dataset\n",
    "    keywords_raw = row[\"Keywords\"]\n",
    "    # Ensure Keywords is a string and not NaN because the data has some missing values in the Keywords column\n",
    "    if pd.notna(keywords_raw):\n",
    "        for keyword in str(keywords_raw).split(\",\"):\n",
    "            # Trouble with keywords containing c(\"\"), so we clean them first\n",
    "            # Clean keywords remove quotes, c(), parentheses so it can match to cuisines\n",
    "            keyword = keyword.strip().replace('\"', '').replace(\"'\", \"\").replace(\"c(\", \"\").replace(\")\", \"\").strip()\n",
    "            \n",
    "            # Skip empty keywords\n",
    "            if not keyword:\n",
    "                continue\n",
    "                \n",
    "            # Check if this keyword appears in any restaurant cuisine ,partial match\n",
    "            matching_cuisines = restaurants_df[restaurants_df['Cuisines'].str.contains(keyword, case=False, na=False, regex=False)]\n",
    "            if len(matching_cuisines) > 0:\n",
    "                # Use a consistent naming convention for the URI\n",
    "                clean_kw = clean_for_uri(keyword)\n",
    "                keywords_uri = FOD[clean_kw]\n",
    "                g.add((recipe_uri, SDO.servesCuisine, keywords_uri))\n",
    "    \n",
    "    if row.name > 10000:\n",
    "        break\n",
    "\n",
    "print(g.serialize(format=\"turtle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb3d86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Itterate over the restaurant data as we did with recipes and nutrition.\n",
    "for _, row in restaurants_df.iterrows():\n",
    "\n",
    "    clean_name = clean_for_uri(row[\"Restaurant Name\"])\n",
    "    restaurant_uri = FOD[clean_name]\n",
    "    \n",
    "    # Adding restaurant instances to the graph with their properties \n",
    "    # such as legal name, aggregate rating, price range, city, country, online delivery and cuisines served\n",
    "    g.add((restaurant_uri, RDF.type, SDO.Restaurant))\n",
    "    g.add((restaurant_uri, SDO.legalName, Literal(row[\"Restaurant Name\"], datatype=XSD.string)))\n",
    "    g.add((restaurant_uri, SDO.AggregateRating, Literal(row[\"Aggregate rating\"], datatype=XSD.decimal)))\n",
    "    g.add((restaurant_uri, SDO.priceRange, Literal(row[\"Price range\"], datatype=XSD.decimal)))\n",
    "    g.add((restaurant_uri, SDO.city, Literal(row[\"City\"], datatype=XSD.string)))\n",
    "    g.add((restaurant_uri, SDO.country, Literal(row[\"Country\"], datatype=XSD.string)))\n",
    "    g.add((restaurant_uri, SDO.orderDelivery, Literal(row[\"Has Online delivery\"], datatype=XSD.integer)))\n",
    "    g.add((restaurant_uri, SDO.servesCuisine, Literal(row[\"Cuisines\"], datatype=XSD.string)))\n",
    "    \n",
    "    if row.name > 10000:\n",
    "        break\n",
    "\n",
    "print(g.serialize(format='ttl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7decbf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the complete structured KG with all entities (recipes, nutrition, restaurants)\n",
    "g.serialize(destination=STRUCTURED_KG, format=\"turtle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64c73d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_REVIEWS = 1000\n",
    "NUM_RECIPES = 1000\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# 1. Build RecipeId -> Recipe Name lookup from first NUM_RECIPES recipes\n",
    "recipe_subset = recipes_df.head(NUM_RECIPES)\n",
    "\n",
    "# clean recipe name for URI\n",
    "recipe_id_to_name = {}\n",
    "for _, row in recipe_subset.iterrows():\n",
    "    recipe_id = str(row[\"RecipeId\"])\n",
    "    recipe_name = clean_for_uri(row[\"Name\"])\n",
    "    recipe_id_to_name[recipe_id] = recipe_name\n",
    "\n",
    "\n",
    "# 2. Vocabulary building for ingredient extraction\n",
    "vocab_set = set()\n",
    "for _, row in recipe_subset.iterrows():\n",
    "    raw_parts = str(row.get(\"RecipeIngredientParts\", \"\"))\n",
    "    parts = re.findall(r'\"([^\"]*)\"', raw_parts) if 'c(' in raw_parts else [raw_parts]\n",
    "    \n",
    "    for p in parts:\n",
    "        doc = nlp(p.lower())\n",
    "        lemmas = [t.lemma_ for t in doc if t.pos_ in [\"NOUN\", \"PROPN\"] and not t.is_stop]\n",
    "        if lemmas: \n",
    "            vocab_set.add(\" \".join(lemmas))\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LEMMA\")\n",
    "matcher.add(\"INGREDIENT\", list(nlp.pipe(vocab_set)))\n",
    "\n",
    "# 3. Review Processing and matching via RecipeId\n",
    "g_unstructured = Graph()\n",
    "g_unstructured.bind(\"schema\", SDO)\n",
    "g_unstructured.bind(\"rec\", REC)\n",
    "g_unstructured.bind(\"fod\", FOD)\n",
    "\n",
    "# Procesing the reviews file line by line\n",
    "matched_reviews = 0\n",
    "with open(REVIEWS_TXT, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\", quotechar='\"')\n",
    "    count = 0\n",
    "    for row in reader:\n",
    "        if count >= MAX_REVIEWS: break\n",
    "        if len(row) < 5: continue\n",
    "        \n",
    "        # Columns: ReviewId, RecipeId, AuthorId, AuthorName, Review, ...\n",
    "        raw_review_id = row[0]\n",
    "        raw_recipe_id = row[1]  # Use RecipeId (not AuthorId!)\n",
    "        review_text = row[4]\n",
    "\n",
    "        # Skip header row\n",
    "        if raw_review_id.lower() == \"reviewid\": continue\n",
    "\n",
    "        # Look up recipe name from RecipeId\n",
    "        if raw_recipe_id not in recipe_id_to_name:\n",
    "            # Recipe not in our subset, skip this review\n",
    "            continue\n",
    "        \n",
    "        recipe_name = recipe_id_to_name[raw_recipe_id]\n",
    "        matched_reviews += 1\n",
    "\n",
    "        safe_rid = clean_for_uri(raw_review_id)\n",
    "        review_uri = FOD[f\"review_{safe_rid}\"]\n",
    "        recipe_uri = REC[recipe_name]\n",
    "        \n",
    "        g_unstructured.add((review_uri, RDF.type, SDO.Review))\n",
    "        g_unstructured.add((review_uri, SDO.UserReview, recipe_uri))\n",
    "        \n",
    "        # Sentiment Analysis\n",
    "        score = analyzer.polarity_scores(review_text)[\"compound\"]\n",
    "        sentiment = \"positive\" if score > 0.05 else \"negative\" if score < -0.05 else \"neutral\"\n",
    "        g_unstructured.add((review_uri, SDO.reviewRating, Literal(sentiment, datatype=XSD.string)))\n",
    "\n",
    "        # Mention extraction\n",
    "        doc = nlp(review_text)\n",
    "        for _, start, end in matcher(doc):\n",
    "            term = doc[start:end].lemma_\n",
    "            g_unstructured.add((review_uri, SDO.keywords, Literal(term, datatype=XSD.string)))\n",
    "\n",
    "        count += 1\n",
    "\n",
    "# 4. Final Serialization\n",
    "try:\n",
    "    g_unstructured.serialize(destination=UNSTRUCTURED_KG, format=\"turtle\")\n",
    "    print(f\"File created successfully: {UNSTRUCTURED_KG}\")\n",
    "except Exception as e:\n",
    "    print(f\"Serialization Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de9db7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "WD = Namespace(\"http://www.wikidata.org/entity/\")\n",
    "WIKIDATA_KG = BASE_PATH + f\"\\\\KEN4256-wikidata-KG-{TEAM_ID}.ttl\"\n",
    "\n",
    "g_wikidata = Graph()\n",
    "g_wikidata.bind(\"wd\", WD)\n",
    "g_wikidata.bind(\"owl\", OWL)\n",
    "g_wikidata.bind(\"schema\", SDO)\n",
    "g_wikidata.bind(\"fod\", FOD)\n",
    "g_wikidata.bind(\"rec\", REC)\n",
    "\n",
    "# Link Ingredients to Wikidata\n",
    "def search_wikidata(term):\n",
    "    \"\"\"Search Wikidata for a term and return QID.\"\"\"\n",
    "    try:\n",
    "        r = requests.get(\"https://www.wikidata.org/w/api.php\",\n",
    "            params={\"action\": \"wbsearchentities\", \"search\": term, \"language\": \"en\", \"format\": \"json\", \"limit\": 1},\n",
    "            headers={\"User-Agent\": \"KG-Course-Assignment/1.0\"},  # <-- THIS LINE WAS MISSING\n",
    "            timeout=10)\n",
    "        data = r.json()\n",
    "        if data.get(\"search\"):\n",
    "            return data[\"search\"][0][\"id\"]\n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "# Extract unique ingredient names from the recipes dataset to link them to Wikidata. \n",
    "# We clean the ingredient names and filter out unreasonable ones to improve matching accuracy.\n",
    "ingredient_words = set()\n",
    "for _, row in recipes_df.head(NUM_RECIPES).iterrows():\n",
    "    raw_parts = str(row.get(\"RecipeIngredientParts\", \"\"))\n",
    "    # Extract individual ingredient names from c(\"...\", \"...\", ...) format\n",
    "    parts = re.findall(r'\"([^\"]*)\"', raw_parts) if 'c(' in raw_parts else [raw_parts]\n",
    "    for p in parts:\n",
    "        # Clean and add if it's a reasonable ingredient name\n",
    "        p = p.strip().lower()\n",
    "        # Allow letters, spaces, and hyphens (e.g. \"vanilla yogurt\", \"lemon juice\")\n",
    "        if len(p) >= 3 and len(p) <= 30 and p.replace(\" \", \"\").replace(\"-\", \"\").isalpha():\n",
    "            ingredient_words.add(p)\n",
    "\n",
    "\n",
    "# Link to Wikidata\n",
    "ingredient_links = 0\n",
    "# We limit to 50 ingredients for demonstration purposes, but in a real scenario we would want to link as many as possible while respecting rate limits and computational resources.\n",
    "for ingredient in list(ingredient_words)[:50]:  # Limit to 50\n",
    "    qid = search_wikidata(ingredient)\n",
    "    if qid:\n",
    "        ing_uri = FOD[f\"ingredient_{clean_for_uri(ingredient)}\"]\n",
    "        g_wikidata.add((ing_uri, RDF.type, SDO.Ingredient))\n",
    "        g_wikidata.add((ing_uri, RDFS.label, Literal(ingredient, lang=\"en\")))\n",
    "        g_wikidata.add((ing_uri, OWL.sameAs, WD[qid]))\n",
    "        ingredient_links += 1\n",
    "        print(f\"  Linked: {ingredient} -> wd:{qid}\")\n",
    "    time.sleep(0.4)  # Rate limiting\n",
    "\n",
    "print(f\"\\nLinked {ingredient_links} ingredients to Wikidata\")\n",
    "\n",
    "# Query Wikidata for Recipe-Cuisine Relationships\n",
    "WIKIDATA_ENDPOINT = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "# Get cuisines and their countries of origin\n",
    "cuisine_query = \"\"\"\n",
    "SELECT ?cuisine ?cuisineLabel ?country ?countryLabel WHERE {\n",
    "  ?cuisine wdt:P31 wd:Q1778821 .\n",
    "  OPTIONAL { ?cuisine wdt:P495 ?country . }\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "} LIMIT 100\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    r = requests.get(WIKIDATA_ENDPOINT, \n",
    "        params={\"query\": cuisine_query}, \n",
    "        headers={\"Accept\": \"application/sparql-results+json\", \"User-Agent\": \"KG-Course-Assignment/1.0\"},\n",
    "        timeout=60)\n",
    "    \n",
    "    if r.status_code == 200:\n",
    "        results = r.json()[\"results\"][\"bindings\"]\n",
    "        for item in results:\n",
    "            qid = item[\"cuisine\"][\"value\"].split(\"/\")[-1]\n",
    "            label = item.get(\"cuisineLabel\", {}).get(\"value\", \"Unknown\")\n",
    "            \n",
    "            cuisine_uri = FOD[f\"cuisine_{clean_for_uri(label)}\"]\n",
    "            g_wikidata.add((cuisine_uri, RDF.type, SDO.Cuisine))\n",
    "            g_wikidata.add((cuisine_uri, RDFS.label, Literal(label, lang=\"en\")))\n",
    "            g_wikidata.add((cuisine_uri, OWL.sameAs, WD[qid]))\n",
    "            \n",
    "            if \"countryLabel\" in item:\n",
    "                country = item[\"countryLabel\"][\"value\"]\n",
    "                g_wikidata.add((cuisine_uri, SDO.country, Literal(country, datatype=XSD.string)))\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"  Error fetching cuisines: {e}\")\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "# Get dishes and their cuisines\n",
    "print(\"\\nFetching dish-cuisine relationships...\")\n",
    "dish_query = \"\"\"\n",
    "SELECT ?dish ?dishLabel ?cuisine ?cuisineLabel WHERE {\n",
    "  ?dish wdt:P31 wd:Q746549 .\n",
    "  ?dish wdt:P361 ?cuisine .\n",
    "  ?cuisine wdt:P31 wd:Q1778821 .\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "} LIMIT 200\n",
    "\"\"\"\n",
    "\n",
    "# We query Wikidata for dishes (instances of Q746549) and their associated cuisines (which are instances of Q1778821).\n",
    "try:\n",
    "    r = requests.get(WIKIDATA_ENDPOINT, \n",
    "        params={\"query\": dish_query}, \n",
    "        headers={\"Accept\": \"application/sparql-results+json\", \"User-Agent\": \"KG-Course-Assignment/1.0\"},\n",
    "        timeout=60)\n",
    "    \n",
    "    # If the request is successful, we process the results and add them to our Wikidata KG. \n",
    "    # We create URIs for dishes and cuisines, link them with the servesCuisine property, and also link to their Wikidata QIDs using owl:sameAs.\n",
    "    if r.status_code == 200:\n",
    "        results = r.json()[\"results\"][\"bindings\"]\n",
    "        for item in results:\n",
    "            dish_qid = item[\"dish\"][\"value\"].split(\"/\")[-1]\n",
    "            dish_label = item.get(\"dishLabel\", {}).get(\"value\", \"Unknown\")\n",
    "            cuisine_label = item.get(\"cuisineLabel\", {}).get(\"value\", \"Unknown\")\n",
    "            \n",
    "            dish_uri = FOD[f\"dish_{clean_for_uri(dish_label)}\"]\n",
    "            cuisine_uri = FOD[f\"cuisine_{clean_for_uri(cuisine_label)}\"]\n",
    "            \n",
    "            g_wikidata.add((dish_uri, RDF.type, SDO.Recipe))\n",
    "            g_wikidata.add((dish_uri, RDFS.label, Literal(dish_label, lang=\"en\")))\n",
    "            g_wikidata.add((dish_uri, OWL.sameAs, WD[dish_qid]))\n",
    "            g_wikidata.add((dish_uri, SDO.servesCuisine, cuisine_uri))\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"  Error fetching dishes: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c28370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge structured, unstructured, and Wikidata KGs into integrated KG\n",
    "integrated_g = Graph()\n",
    "integrated_g.parse(STRUCTURED_KG, format=\"turtle\")\n",
    "\n",
    "# We attempt to load the unstructured and Wikidata KGs, but we handle potential errors gracefully. \n",
    "# If there are issues with loading these KGs, we catch the exceptions and print a note instead of crashing the program.\n",
    "try:\n",
    "    integrated_g.parse(UNSTRUCTURED_KG, format=\"turtle\")\n",
    "    print(f\"Loaded unstructured KG. Total triples: {len(integrated_g)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: Could not load unstructured KG: {e}\")\n",
    "\n",
    "try:\n",
    "    integrated_g.parse(WIKIDATA_KG, format=\"turtle\")\n",
    "    print(f\"Loaded Wikidata KG. Total triples: {len(integrated_g)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: Could not load Wikidata KG: {e}\")\n",
    "\n",
    "integrated_g.serialize(destination=INTEGRATED_KG, format=\"turtle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6500f07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"LOADING THE GRAPH (This may take 10-20 seconds) ...\")\n",
    "g = rdflib.Graph()\n",
    "try:\n",
    "    g.parse(INTEGRATED_KG, format=\"turtle\")\n",
    "    print(f\"Graph ready! Total triples: {len(g)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# PREFIXES\n",
    "PREFIXES = \"\"\"\n",
    "    PREFIX fod: <http://kg-course.io/food-nutrition/>\n",
    "    PREFIX rec: <http://kg-course.io/food-nutrition/recipe/>\n",
    "    PREFIX nutr: <http://kg-course.io/food-nutrition/nutrition/>\n",
    "    PREFIX schema: <https://schema.org/>\n",
    "    PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "\"\"\"\n",
    "\n",
    "# Open the text file to write results\n",
    "with open(SPARQL_RESULTS, \"w\", encoding=\"utf-8\") as f:\n",
    "    \n",
    "    def run_query(task_id, description, query_text):\n",
    "        print(f\"\\n>>> Processing Task {task_id}: {description}...\")\n",
    "        \n",
    "        # Write Header to file\n",
    "        f.write(f\"\\n{'='*50}\\n\")\n",
    "        f.write(f\"Task {task_id}: {description}\\n\")\n",
    "        f.write(f\"{'='*50}\\n\")\n",
    "        \n",
    "        try:\n",
    "            t0 = time.time()\n",
    "            results = g.query(PREFIXES + query_text)\n",
    "            dt = time.time() - t0\n",
    "            \n",
    "            data = []\n",
    "            for row in results:\n",
    "                clean_row = []\n",
    "                for item in row:\n",
    "                    if item is None:\n",
    "                        clean_row.append(\"N/A\")\n",
    "                    else: \n",
    "                        val = str(item)\n",
    "                        # Keep only actual image URLs intact (not KG URIs)\n",
    "                        if ('http://' in val or 'https://' in val) and ('.jpg' in val or '.png' in val):\n",
    "                            clean_row.append(val)\n",
    "                        elif val.strip() == '':\n",
    "                            clean_row.append(\"\")\n",
    "                        else:\n",
    "                            clean_row.append(val.split('/')[-1].replace(\"_\", \" \"))\n",
    "                data.append(clean_row)\n",
    "                \n",
    "            cols = [str(v) for v in results.vars]\n",
    "            df = pd.DataFrame(data, columns=cols)\n",
    "            \n",
    "            if df.empty:\n",
    "                msg = \"(No results found in this sample)\"\n",
    "                print(f\"   {msg}\")\n",
    "                f.write(f\"{msg}\\n\")\n",
    "            else:\n",
    "                # Convert DataFrame to string table\n",
    "                table_str = df.head(20).to_string(index=False)\n",
    "                print(df.head(5).to_string(index=False)) # Show preview in terminal\n",
    "                f.write(table_str + \"\\n\") # Write full table to file\n",
    "                print(f\"   (Done in {dt:.2f}s)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            err_msg = f\"Error: {e}\"\n",
    "            print(f\"   {err_msg}\")\n",
    "            f.write(f\"{err_msg}\\n\")\n",
    "\n",
    "    # QUERIES\n",
    "\n",
    "    # 4.1\n",
    "    run_query(\"4.1\", \"Recipes with Mango\", \"\"\"\n",
    "    SELECT ?Name WHERE {\n",
    "        ?recipe a schema:Recipe ; schema:givenName ?Name ; schema:ingredients ?ing .\n",
    "        FILTER(CONTAINS(LCASE(?ing), \"mango\"))\n",
    "    } LIMIT 30\n",
    "    \"\"\")\n",
    "\n",
    "    # 4.2\n",
    "    run_query(\"4.2\", \"Quick and Healthy Pies\", \"\"\"\n",
    "    SELECT ?Name ?Time WHERE {\n",
    "        ?recipe a schema:Recipe ; schema:givenName ?Name ; schema:keywords ?k ; schema:cookTime ?Time .\n",
    "        FILTER(CONTAINS(LCASE(?k), \"healthy\") && CONTAINS(LCASE(?Name), \"pie\"))\n",
    "        FILTER(!CONTAINS(?Time, \"H\"))\n",
    "        FILTER(!CONTAINS(LCASE(STR(?Time)), \"nan\"))\n",
    "    } LIMIT 30\n",
    "    \"\"\")\n",
    "\n",
    "    # 4.3\n",
    "    run_query(\"4.3\", \"Chinese Restaurants New Delhi\", \"\"\"\n",
    "    SELECT ?Restaurant WHERE {\n",
    "        ?rest a schema:Restaurant ; schema:legalName ?Restaurant ; \n",
    "            schema:city ?city ; schema:orderDelivery ?delivery ; schema:servesCuisine ?c .\n",
    "        FILTER(STR(?city) = \"New Delhi\")\n",
    "        FILTER(?delivery = 1)\n",
    "        FILTER(CONTAINS(LCASE(STR(?c)), \"chinese\"))\n",
    "    } LIMIT 10\n",
    "    \"\"\")\n",
    "\n",
    "    # 4.4\n",
    "    run_query(\"4.4\", \"Average Cost (Davenport Asian)\", \"\"\"\n",
    "    SELECT (AVG(?price) AS ?Average_Cost) WHERE {\n",
    "        ?rest a schema:Restaurant ; schema:city ?city ; \n",
    "            schema:priceRange ?price ; schema:servesCuisine ?c .\n",
    "        FILTER(STR(?city) = \"Davenport\")\n",
    "        FILTER(REGEX(STR(?c), \"Asian|Chinese|Thai\", \"i\"))\n",
    "    }\n",
    "    \"\"\")\n",
    "\n",
    "    # 4.5\n",
    "    run_query(\"4.5\", \"Easy Desserts (<300cal, after 2000)\", \"\"\"\n",
    "    SELECT ?Name ?Calories ?Image WHERE {\n",
    "        ?recipe a schema:Recipe ; schema:givenName ?Name ; schema:recipeCategory ?cat ; \n",
    "                schema:keywords ?k ; schema:hasNutrition ?n ; schema:image ?Image ; schema:datePublished ?date .\n",
    "        ?n schema:calories ?Calories .\n",
    "        FILTER(REGEX(STR(?cat), \"dessert\", \"i\") && REGEX(STR(?k), \"easy\", \"i\") && ?Calories < 300)\n",
    "        FILTER(CONTAINS(STR(?date), \"200\") || CONTAINS(STR(?date), \"201\") || CONTAINS(STR(?date), \"202\"))\n",
    "    } ORDER BY ASC(?Calories) LIMIT 5\n",
    "    \"\"\")\n",
    "\n",
    "    # 4.6\n",
    "    run_query(\"4.6\", \"Top Beverages (Sentiment)\", \"\"\"\n",
    "    SELECT ?Name ?PrepTime ?Sugar (SAMPLE(?Sentiment) AS ?SentimentSample) WHERE {\n",
    "        ?recipe a schema:Recipe ; schema:givenName ?Name ; schema:recipeCategory ?cat ; \n",
    "                schema:cookTime ?PrepTime ; schema:hasNutrition ?n .\n",
    "        ?n schema:sugarContent ?Sugar .\n",
    "        ?review a schema:Review ; schema:UserReview ?recipe ; schema:reviewRating ?Sentiment .\n",
    "        FILTER(CONTAINS(LCASE(?cat), \"beverage\"))\n",
    "    } GROUP BY ?Name ?PrepTime ?Sugar\n",
    "    ORDER BY DESC(?Sugar) LIMIT 10\n",
    "    \"\"\")\n",
    "\n",
    "    # 4.7\n",
    "    run_query(\"4.7\", \"High Protein Recipes\", \"\"\"\n",
    "    SELECT ?Name ?Protein ?Cuisine WHERE {\n",
    "        ?recipe a schema:Recipe ; schema:givenName ?Name ; schema:hasNutrition ?nut .\n",
    "        ?nut schema:proteinContent ?Protein .\n",
    "        FILTER(?Protein > 20)\n",
    "        OPTIONAL {\n",
    "            ?recipe schema:servesCuisine ?cuisineUri .\n",
    "            BIND(STR(?cuisineUri) AS ?Cuisine)\n",
    "        }\n",
    "    } ORDER BY DESC(?Protein) LIMIT 10\n",
    "    \"\"\")\n",
    "\n",
    "    # 4.8\n",
    "    run_query(\"4.8\", \"Top Healthy (NDS Score)\", \"\"\"\n",
    "    SELECT ?Name ?NDS ?Protein ?Fiber ?Sugar WHERE {\n",
    "        ?recipe a schema:Recipe ; schema:givenName ?Name ; schema:hasNutrition ?n .\n",
    "        ?n schema:proteinContent ?p ; schema:fiberContent ?f ; schema:sugarContent ?s .\n",
    "        BIND(?p AS ?Protein)\n",
    "        BIND(?f AS ?Fiber)\n",
    "        BIND(?s AS ?Sugar)\n",
    "        BIND ((1.0 * ?p + 1.5 * ?f - 2.0 * ?s) AS ?NDS)\n",
    "    } ORDER BY DESC(?NDS) LIMIT 5\n",
    "    \"\"\")\n",
    "\n",
    "print(f\"\\n SUCCESS! Results saved to '{SPARQL_RESULTS}' \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
